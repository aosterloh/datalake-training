{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J3TwAK_8Wll0"
   },
   "source": [
    "## 3. Data Scientist - Create ML models with Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "rwCbd6SqWll0"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import DecisionTreeClassifier, LogisticRegression\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, f1_score\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator, RegressionEvaluator\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "g7hujos_zCwi"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    ".appName('Spark - Data Scientist Demo') \\\n",
    ".config('spark.jars', 'gs://spark-lib/bigquery/spark-bigquery-latest.jar') \\\n",
    ".config(\"spark.jars.packages\", \"com.google.cloud.spark:spark-bigquery-with-dependencies_2.12:0.18.0\") \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "mZAK-gbxzCwj"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'version 2.12.10'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.app.id\")\n",
    "spark.sparkContext._jvm.scala.util.Properties.versionString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ylnHDJ13zCwj"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'thetraining-project:thetraining_project_raw.transaction_data_train'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project_id = !gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "bq_raw_dataset_name = project_id[0] + '-raw'\n",
    "bq_raw_dataset_name = bq_raw_dataset_name.replace('-', '_')\n",
    "bq_raw_table_path = project_id[0] + ':' + bq_raw_dataset_name + '.transaction_data_train' \n",
    "bq_raw_table_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "asGBbiH5zCwk"
   },
   "source": [
    "#### Load Training Data using Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "p20yxBKHzCwl"
   },
   "outputs": [],
   "source": [
    "data = spark.read \\\n",
    ".format(\"bigquery\") \\\n",
    ".option(\"table\", bq_raw_table_path) \\\n",
    ".load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_y4K036kzCwn"
   },
   "outputs": [],
   "source": [
    "data = data.drop('transactionID')\n",
    "data.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oGTkXZhtzCwn"
   },
   "source": [
    "#### Create a pyspark ML pipeline \n",
    "\n",
    "The pipeline will transform the features and train a Decision Tree classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2kg4ewsvzCwo"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier, DecisionTreeClassifier\n",
    "\n",
    "\n",
    "categorical_cols = [field for (field, data_type) in data.dtypes \n",
    "                    if ((data_type == \"string\") & (field != 'isFraud'))]\n",
    "\n",
    "ohe_output_cols = [x + \"_OHE\" for x in categorical_cols]\n",
    "\n",
    "string_indexers = StringIndexer(inputCol='type', outputCol='type' +\"_Index\").fit(data) \n",
    "\n",
    "one_hot_indexer = OneHotEncoder(inputCol='type_Index', outputCol='type' +\"_OHE\")\n",
    "\n",
    "numeric_cols = [field for (field, data_type) in data.dtypes \n",
    "                if (((data_type == \"double\") | (data_type == \"int\") | (data_type == \"bigint\"))\n",
    "                  & (field != 'isFraud'))]\n",
    "\n",
    "assembler_inputs = ohe_output_cols + numeric_cols\n",
    "\n",
    "vec_assembler = VectorAssembler(\n",
    "    inputCols=assembler_inputs,\n",
    "    outputCol=\"features\")\n",
    "\n",
    "dtc = DecisionTreeClassifier(labelCol=\"isFraud\", featuresCol=\"features\", maxDepth=3, maxBins=12)\n",
    "\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    string_indexers,\n",
    "    one_hot_indexer,\n",
    "    vec_assembler,\n",
    "    dtc \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Selection, Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "--lPFc7mzCwp"
   },
   "source": [
    "#### Train the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "kGU9h1-rzCwq"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-d74f2dc32c8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "model = pipeline.fit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vqH9COddzCwq"
   },
   "source": [
    "#### Persist the model to GCS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cnoutgo2zCwq"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "\n",
    "gcs_bucket = project_id[0] + '-data'\n",
    "model_path = f'gs://{gcs_bucket}/model/'\n",
    "\n",
    "model.write().overwrite().save(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict on test data\n",
    "**TODO** \n",
    "\n",
    "* Provide path_to_predict_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DdasXGm7zCws"
   },
   "outputs": [],
   "source": [
    "path_to_predict_csv = \"gs://thetraining-project-data/transaction_data_test.csv\"\n",
    "df_transaction_data_predict_from_csv = spark \\\n",
    ".read \\\n",
    ".option(\"inferSchema\" , \"true\") \\\n",
    ".option(\"header\" , \"true\") \\\n",
    ".csv(path_to_predict_csv)\n",
    "df_transaction_data_predict_from_csv.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LRp9QM2jzCws"
   },
   "source": [
    "Load the saved model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YFjQsf5pzCws"
   },
   "outputs": [],
   "source": [
    "loaded_pipeline_model = PipelineModel.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wd2X6ZbBzCwt"
   },
   "outputs": [],
   "source": [
    "predictions = loaded_pipeline_model.transform(df_transaction_data_predict_from_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BaR6_kfazCwt"
   },
   "outputs": [],
   "source": [
    "predictions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2FFk6q7fzCwt"
   },
   "outputs": [],
   "source": [
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"isFraud\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TBMiTQJqzCwu"
   },
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_KYod7wWzCwu"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "binaryEvaluator = BinaryClassificationEvaluator(labelCol=\"isFraud\")\n",
    "\n",
    "auc = binaryEvaluator.evaluate(predictions, {binaryEvaluator.metricName: \"areaUnderROC\"})\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FXJV0AQgzCwu"
   },
   "outputs": [],
   "source": [
    "tests_np = np.array((predictions.select(\"isFraud\",\"prediction\").collect()))\n",
    "tests_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SCbijvh6zCwv"
   },
   "outputs": [],
   "source": [
    "tests_np = np.array((predictions.select(\"isFraud\",\"prediction\").collect()))\n",
    "\n",
    "np_acc = accuracy_score(tests_np[:,0], tests_np[:,1])\n",
    "np_f1 = f1_score(tests_np[:,0], tests_np[:,1])\n",
    "np_precision = precision_score(tests_np[:,0], tests_np[:,1])\n",
    "np_recall = recall_score(tests_np[:,0], tests_np[:,1])\n",
    "np_auc = roc_auc_score(tests_np[:,0], tests_np[:,1])\n",
    "\n",
    "print(\"f1:\", np_f1)\n",
    "print(\"precision:\", np_precision)\n",
    "print(\"recall:\", np_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PlDr0C5DzCwv"
   },
   "source": [
    "#### Create confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D0-_eqAszCwv"
   },
   "outputs": [],
   "source": [
    "# import package that will generate the confusion matrix scores\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# import packages that will help display the scores\n",
    "import pandas as pd\n",
    "\n",
    "confusion_matrix_scores = confusion_matrix(tests_np[:,0], \n",
    "                                           tests_np[:,1], \n",
    "                                           labels=[1, 0])\n",
    "\n",
    "# display scores as a heatmap\n",
    "df = pd.DataFrame(confusion_matrix_scores, \n",
    "                  columns = [\"Predicted Positive(1)\", \"Predicted Negative(0)\"],\n",
    "                  index = [\"Actual Positive(1)\", \"Actual Negative(0)\"])\n",
    "\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "8Q3YHVvezCww"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'thetraining_project_annotated.transaction_data_predictions'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bq_annotated_table_name = 'transaction_data_predictions'\n",
    "bq_annotated_table_path=  project_id[0] +  '_annotated.' + bq_annotated_table_name\n",
    "bq_annotated_table_path = bq_annotated_table_path.replace('-', '_')\n",
    "bq_annotated_table_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cDn4bdq1zCww"
   },
   "source": [
    "#### Persist predictions as an annotated dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "vwfUAozUzCwx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BigQuery error in mk operation: Table 'thetraining-\n",
      "project:thetraining_project_annotated.transaction_data_predictions' could not be\n",
      "created; a table with this name already exists.\n"
     ]
    }
   ],
   "source": [
    "schema_inline = predictions.schema.simpleString().replace('struct<', '').replace('>', '').replace('int', 'int64').replace('double', 'float64').replace('bigint64', 'int64').replace('vector', 'STRING')\n",
    "\n",
    "!bq mk --table \\\n",
    "{bq_annotated_table_path} \\\n",
    "{schema_inline}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "mKDzZWhTzCwx"
   },
   "outputs": [],
   "source": [
    "predictions.write \\\n",
    ".format(\"bigquery\") \\\n",
    ".option(\"table\", project_id[0]  + ':' + bq_annotated_table_path) \\\n",
    ".option(\"temporaryGcsBucket\", project_id[0]  + '-data') \\\n",
    ".mode('overwrite') \\\n",
    ".save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "B6EreECmzCwy"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'thetraining_project_annotated'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotated_dataset_name =  project_id[0] +  '_annotated'\n",
    "annotated_dataset_name = annotated_dataset_name.replace('-', '_')\n",
    "annotated_dataset_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OGwxhnHGzCwy"
   },
   "source": [
    "**TODO** \n",
    "* Add annotated_dataset_name in the FROM clause below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "2-gJPCtxzCwz"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>table_catalog</th>\n",
       "      <th>table_schema</th>\n",
       "      <th>table_name</th>\n",
       "      <th>table_type</th>\n",
       "      <th>is_insertable_into</th>\n",
       "      <th>is_typed</th>\n",
       "      <th>creation_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>thetraining-project</td>\n",
       "      <td>thetraining_project_annotated</td>\n",
       "      <td>transaction_data_predictions</td>\n",
       "      <td>BASE TABLE</td>\n",
       "      <td>YES</td>\n",
       "      <td>NO</td>\n",
       "      <td>2021-03-03 09:56:08.781000+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         table_catalog                   table_schema  \\\n",
       "0  thetraining-project  thetraining_project_annotated   \n",
       "\n",
       "                     table_name  table_type is_insertable_into is_typed  \\\n",
       "0  transaction_data_predictions  BASE TABLE                YES       NO   \n",
       "\n",
       "                     creation_time  \n",
       "0 2021-03-03 09:56:08.781000+00:00  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "SELECT * FROM thetraining_project_annotated.INFORMATION_SCHEMA.TABLES;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wbZpvQ9nzCwz"
   },
   "source": [
    "#### Join buisness data to enrich the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k00qmMINzCwz"
   },
   "source": [
    "**TODO** \n",
    "* Provide the path to the join csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "zf_shWXdzCw0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- nameOrig: string (nullable = true)\n",
      " |-- nameDest: string (nullable = true)\n",
      " |-- transactionID: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path_to_join_csv = \"gs://thetraining-project-data/transaction_data_join.csv\"\n",
    "df_transaction_data_join_from_csv = spark \\\n",
    ".read \\\n",
    ".option(\"inferSchema\" , \"true\") \\\n",
    ".option(\"header\" , \"true\") \\\n",
    ".csv(path_to_join_csv)\n",
    "df_transaction_data_join_from_csv.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mjGflyBOzCw0"
   },
   "source": [
    "**TODO** (Challenge 2)\n",
    "* Join the 2 spark dataframes (predictions & df_transaction_data_join_from_csv) on transactionID field "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "pm-AH24IzCw1"
   },
   "outputs": [],
   "source": [
    "joined_result = predictions.join(df_transaction_data_join_from_csv, \"transactionID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "vFE7tsZczCw1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+--------+---------+-------------+--------------+--------------+--------------+-------+----------+-------------+--------------------+------------------+--------------------+----------+-----------+-----------+\n",
      "|       transactionID|step|    type|   amount|oldbalanceOrg|newbalanceOrig|oldbalanceDest|newbalanceDest|isFraud|type_Index|     type_OHE|            features|     rawPrediction|         probability|prediction|   nameOrig|   nameDest|\n",
      "+--------------------+----+--------+---------+-------------+--------------+--------------+--------------+-------+----------+-------------+--------------------+------------------+--------------------+----------+-----------+-----------+\n",
      "|0013918d-0cdc-4e2...| 306|   DEBIT|  1244.34|      30927.0|      29682.66|    3248801.84|    3250046.18|      0|       4.0|    (4,[],[])|[0.0,0.0,0.0,0.0,...|[4661108.0,2869.0]|[0.99938485974523...|       0.0| C831886474|C1847064333|\n",
      "|00162366-932d-4fc...| 204|CASH_OUT|203860.23|          0.0|           0.0|     336504.92|     540365.16|      0|       0.0|(4,[0],[1.0])|(10,[0,4,5,8,9],[...|[4661108.0,2869.0]|[0.99938485974523...|       0.0| C896728022| C331131087|\n",
      "|0022ef6a-0ee3-4e8...|  13|CASH_OUT| 382105.5|          0.0|           0.0|    2750925.15|    3133030.66|      0|       0.0|(4,[0],[1.0])|(10,[0,4,5,8,9],[...|[4661108.0,2869.0]|[0.99938485974523...|       0.0|C1150532907|C1660526430|\n",
      "|003402a4-bc23-4f2...| 178| PAYMENT| 27994.13|      69545.0|      41550.87|           0.0|           0.0|      0|       1.0|(4,[1],[1.0])|(10,[1,4,5,6,7],[...|[4661108.0,2869.0]|[0.99938485974523...|       0.0|C1260774996| M740485457|\n",
      "|0036a79e-5f07-4ec...| 209|CASH_OUT|329965.55|        456.0|           0.0|           0.0|     329965.55|      0|       0.0|(4,[0],[1.0])|(10,[0,4,5,6,9],[...|[4661108.0,2869.0]|[0.99938485974523...|       0.0|C1471278494| C408482446|\n",
      "+--------------------+----+--------+---------+-------------+--------------+--------------+--------------+-------+----------+-------------+--------------------+------------------+--------------------+----------+-----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined_result.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "a2uQsRCbzCw2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1271928"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined_result.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9K33zizTzCw2"
   },
   "source": [
    "#### Persist result as an enriched dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "QsiOiwSOzCw2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'thetraining-project:thetraining_project_enriched.transaction_analysis_enriched'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bq_enriched_table_name = 'transaction_analysis_enriched'\n",
    "bq_enriched_table_path = project_id[0] +  '_enriched.' + bq_enriched_table_name\n",
    "bq_enriched_table_path = bq_enriched_table_path.replace('-', '_')\n",
    "bq_enriched_table_path = project_id[0] + ':' + bq_enriched_table_path\n",
    "bq_enriched_table_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "6zl_7sFOzCw2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BigQuery error in mk operation: Table 'thetraining-\n",
      "project:thetraining_project_enriched.transaction_analysis_enriched' could not be\n",
      "created; a table with this name already exists.\n"
     ]
    }
   ],
   "source": [
    "schema_inline = joined_result.schema.simpleString().replace('struct<', '').replace('>', '').replace('int', 'int64').replace('bigint64', 'int64').replace('double', 'float64').replace('vector', 'STRING')\n",
    "\n",
    "!bq mk --table \\\n",
    "{bq_enriched_table_path} \\\n",
    "{schema_inline}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'transactionID:string,step:int64,type:string,amount:float64,oldbalanceOrg:float64,newbalanceOrig:float64,oldbalanceDest:float64,newbalanceDest:float64,isFraud:int64,type_Index:float64,type_OHE:STRING,features:STRING,rawPrediction:STRING,probability:STRING,prediction:float64,nameOrig:string,nameDest:string'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema_inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "xG1fvtj6zCw3"
   },
   "outputs": [],
   "source": [
    "joined_result.write \\\n",
    ".format(\"bigquery\") \\\n",
    ".option(\"table\", bq_enriched_table_path) \\\n",
    ".option(\"temporaryGcsBucket\", project_id[0]  + '-data') \\\n",
    ".mode('overwrite') \\\n",
    ".save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "dN-MyDCszCw3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'thetraining_project_enriched'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enriched_dataset_name = project_id[0] +  '_enriched'\n",
    "enriched_dataset_name = enriched_dataset_name.replace('-', '_')\n",
    "enriched_dataset_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qtXtLcrXzCw3"
   },
   "source": [
    "**TODO**\n",
    "* Provide the enriched_dataset_name in the FROM clause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "nbiRVokHzCw4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>table_catalog</th>\n",
       "      <th>table_schema</th>\n",
       "      <th>table_name</th>\n",
       "      <th>table_type</th>\n",
       "      <th>is_insertable_into</th>\n",
       "      <th>is_typed</th>\n",
       "      <th>creation_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>thetraining-project</td>\n",
       "      <td>thetraining_project_enriched</td>\n",
       "      <td>transaction_analysis_enriched</td>\n",
       "      <td>BASE TABLE</td>\n",
       "      <td>YES</td>\n",
       "      <td>NO</td>\n",
       "      <td>2021-03-03 12:09:03.854000+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         table_catalog                  table_schema  \\\n",
       "0  thetraining-project  thetraining_project_enriched   \n",
       "\n",
       "                      table_name  table_type is_insertable_into is_typed  \\\n",
       "0  transaction_analysis_enriched  BASE TABLE                YES       NO   \n",
       "\n",
       "                     creation_time  \n",
       "0 2021-03-03 12:09:03.854000+00:00  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "SELECT * FROM thetraining_project_enriched.INFORMATION_SCHEMA.TABLES;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**\n",
    "* Query the enriched table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UY96LzmUzCw4"
   },
   "source": [
    "**TODO** (Optional: Challenge 3)\n",
    "* Improve the ML pipeline\n",
    "    * Try out different ML models [[doc]](https://spark.apache.org/docs/latest/ml-pipeline.html)\n",
    "    * Explore hyperparameter tuning \n",
    "    * How would you split the data when there is class imbalance? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "### Create a pyspark ML pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = [field for (field, data_type) in data.dtypes \n",
    "                    if ((data_type == \"string\") & (field != 'isFraud'))]\n",
    "\n",
    "ohe_output_cols = [x + \"_OHE\" for x in categorical_cols]\n",
    "\n",
    "#StringIndexer encodes a string column of labels to a column of label indices.\n",
    "string_indexers = StringIndexer(inputCol='type', outputCol='type' +\"_Index\").fit(data) \n",
    "\n",
    "\n",
    "#One-hot encoding maps a categorical feature, represented as a label index,\n",
    "#to a binary vector with at most a single one-value indicating the presence\n",
    "#of a specific feature value from among the set of all feature values. \n",
    "one_hot_indexer = OneHotEncoder(inputCol='type_Index', outputCol='type' +\"_OHE\")\n",
    "\n",
    "numeric_cols = [field for (field, data_type) in data.dtypes \n",
    "                if (((data_type == \"double\") | (data_type == \"int\") | (data_type == \"bigint\"))\n",
    "                  & (field != 'isFraud'))]\n",
    "\n",
    "assembler_inputs = ohe_output_cols + numeric_cols\n",
    "\n",
    "#VectorAssembler is a transformer that combines a given list of columns into a single vector column.\n",
    "vec_assembler = VectorAssembler(\n",
    "    inputCols=assembler_inputs,\n",
    "    outputCol=\"features\")\n",
    "\n",
    "lr = LogisticRegression(labelCol=\"isFraud\", featuresCol=\"features\", maxIter=10, )\n",
    "\n",
    "pipeline_lr = Pipeline(stages=[\n",
    "    string_indexers,\n",
    "    one_hot_indexer,\n",
    "    vec_assembler,\n",
    "    lr \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "regModel= pipeline_lr.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "\n",
    "gcs_bucket = project_id[0] + '-data'\n",
    "model_path = f'gs://{gcs_bucket}/regmodel/'\n",
    "\n",
    "regModel.write().overwrite().save(model_path_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- step: integer (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      " |-- oldbalanceOrg: double (nullable = true)\n",
      " |-- newbalanceOrig: double (nullable = true)\n",
      " |-- oldbalanceDest: double (nullable = true)\n",
      " |-- newbalanceDest: double (nullable = true)\n",
      " |-- isFraud: integer (nullable = true)\n",
      " |-- transactionID: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path_to_predict_csv = \"gs://thetraining-project-data/transaction_data_test.csv\"\n",
    "df_transaction_data_predict_from_csv_lr = spark \\\n",
    ".read \\\n",
    ".option(\"inferSchema\" , \"true\") \\\n",
    ".option(\"header\" , \"true\") \\\n",
    ".csv(path_to_predict_csv)\n",
    "df_transaction_data_predict_from_csv_lr.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_pipeline_model_lr = PipelineModel.load(model_path_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_lr = loaded_pipeline_model_lr.transform(df_transaction_data_predict_from_csv_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-------+-------------+--------------+--------------+--------------+-------+--------------------+----------+---------+--------------------+------------------+--------------------+----------+\n",
      "|step| type| amount|oldbalanceOrg|newbalanceOrig|oldbalanceDest|newbalanceDest|isFraud|       transactionID|type_Index| type_OHE|            features|     rawPrediction|         probability|prediction|\n",
      "+----+-----+-------+-------------+--------------+--------------+--------------+-------+--------------------+----------+---------+--------------------+------------------+--------------------+----------+\n",
      "| 310|DEBIT|3334.31|     102439.0|      99104.69|     962290.61|     965624.92|      0|818ca43a-9ac9-484...|       4.0|(4,[],[])|[0.0,0.0,0.0,0.0,...|[4661108.0,2869.0]|[0.99938485974523...|       0.0|\n",
      "| 351|DEBIT|5268.56|        468.0|           0.0|    1672373.71|    1677642.27|      0|70948920-5fa6-401...|       4.0|(4,[],[])|(10,[4,5,6,8,9],[...|[4661108.0,2869.0]|[0.99938485974523...|       0.0|\n",
      "| 592|DEBIT| 3016.8|      62038.0|       59021.2|    9669792.89|    9672809.69|      0|c6bbbaf0-55bb-4e9...|       4.0|(4,[],[])|[0.0,0.0,0.0,0.0,...|[4661108.0,2869.0]|[0.99938485974523...|       0.0|\n",
      "| 286|DEBIT|2005.45|     462875.0|     460869.55|     757666.15|      759671.6|      0|a8b178e2-6e70-42f...|       4.0|(4,[],[])|[0.0,0.0,0.0,0.0,...|[4661108.0,2869.0]|[0.99938485974523...|       0.0|\n",
      "| 157|DEBIT|7422.22|      20639.0|      13216.78|    3957547.41|    3964969.63|      0|9931bfd1-0b59-4d3...|       4.0|(4,[],[])|[0.0,0.0,0.0,0.0,...|[4661108.0,2869.0]|[0.99938485974523...|       0.0|\n",
      "+----+-----+-------+-------------+--------------+--------------+--------------+-------+--------------------+----------+---------+--------------------+------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_lr.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.44920580727860715\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "regEvaluator = RegressionEvaluator(labelCol=\"isFraud\")\n",
    "\n",
    "r2_lr = regEvaluator.evaluate(predictions_lr, {regEvaluator.metricName: \"r2\"})\n",
    "print(r2_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       ...,\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.]])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tests_np_lr = np.array((predictions_lr.select(\"isFraud\",\"prediction\").collect()))\n",
    "tests_np_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: 0.6783746556473829\n",
      "precision: 0.8167495854063018\n",
      "recall: 0.5800942285041225\n"
     ]
    }
   ],
   "source": [
    "np_acc_lr = accuracy_score(tests_np_lr[:,0], tests_np_lr[:,1])\n",
    "np_f1_lr = f1_score(tests_np_lr[:,0], tests_np_lr[:,1])\n",
    "np_precision_lr = precision_score(tests_np_lr[:,0], tests_np_lr[:,1])\n",
    "np_recall_lr = recall_score(tests_np_lr[:,0], tests_np_lr[:,1])\n",
    "np_auc_lr = roc_auc_score(tests_np_lr[:,0], tests_np_lr[:,1])\n",
    "\n",
    "print(\"f1:\", np_f1_lr)\n",
    "print(\"precision:\", np_precision_lr)\n",
    "print(\"recall:\", np_recall_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted Positive(1)</th>\n",
       "      <th>Predicted Negative(0)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual Positive(1)</th>\n",
       "      <td>985</td>\n",
       "      <td>713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual Negative(0)</th>\n",
       "      <td>221</td>\n",
       "      <td>1270009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Predicted Positive(1)  Predicted Negative(0)\n",
       "Actual Positive(1)                    985                    713\n",
       "Actual Negative(0)                    221                1270009"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix_scores_lr = confusion_matrix(tests_np_lr[:,0], \n",
    "                                           tests_np_lr[:,1], \n",
    "                                           labels=[1, 0])\n",
    "# display scores as a heatmap\n",
    "df_lr = pd.DataFrame(confusion_matrix_scores_lr, \n",
    "                  columns = [\"Predicted Positive(1)\", \"Predicted Negative(0)\"],\n",
    "                  index = [\"Actual Positive(1)\", \"Actual Negative(0)\"])\n",
    "df_lr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection\n",
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DecisionTrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "paramGrid_dtc = ParamGridBuilder() \\\n",
    "    .addGrid(dtc.maxDepth, [3,6,10,20]) \\\n",
    "    .addGrid(dtc.maxBins, [12,24,36]) \\\n",
    "    .build()\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid_dtc,\n",
    "                          evaluator=BinaryClassificationEvaluator(labelCol=\"isFraud\"),\n",
    "                          numFolds=2)  # use 3+ folds in practice\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "tunedDtcModel = crossval.fit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "paramGrid_lr = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "    .build()\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline_lr,\n",
    "                          estimatorParamMaps=paramGrid_lr,\n",
    "                          evaluator=RegressionEvaluator(labelCol=\"isFraud\"),\n",
    "                          numFolds=2)\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "tunedRegModel = crossval.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio: 780\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, explode, array, lit\n",
    "\n",
    "major_df = data.filter(col(\"isFraud\") == 0)\n",
    "minor_df = data.filter(col(\"isFraud\") == 1)\n",
    "ratio = int(major_df.count()/minor_df.count())\n",
    "print(\"ratio: {}\".format(ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+----------+-------------+--------------+--------------+--------------+-------+\n",
      "|step|    type|    amount|oldbalanceOrg|newbalanceOrig|oldbalanceDest|newbalanceDest|isFraud|\n",
      "+----+--------+----------+-------------+--------------+--------------+--------------+-------+\n",
      "| 228|TRANSFER|1889104.07|      50211.0|           0.0|    1013577.63|     2902681.7|      0|\n",
      "| 474|TRANSFER| 519209.54|       6455.0|           0.0|     939163.84|    1458373.38|      0|\n",
      "| 405|TRANSFER| 659060.17|         57.0|           0.0|     921090.33|     1580150.5|      0|\n",
      "|  34|TRANSFER| 293246.02|    307940.78|      14694.76|     1546198.1|    1839444.12|      0|\n",
      "|  33|TRANSFER|  35686.42|     105145.0|      69458.58|     426298.77|     461985.19|      0|\n",
      "| 328|TRANSFER|4645239.06|        200.0|           0.0|      47404.18|    4692643.24|      0|\n",
      "| 232|TRANSFER|1627117.36|     13291.46|           0.0|    2005786.89|    3632904.25|      0|\n",
      "| 681|TRANSFER| 614053.95|      43654.0|           0.0|    1960532.96|     2574586.9|      0|\n",
      "| 542|TRANSFER| 1403621.2|      60581.0|           0.0|    1867470.86|    3271092.05|      0|\n",
      "|  94|TRANSFER| 203034.16|         53.0|           0.0|       6569.57|     209603.73|      0|\n",
      "| 349|TRANSFER|2335522.37|      65272.0|           0.0|    4043699.93|     6379222.3|      0|\n",
      "| 359|TRANSFER|2490479.07|      5967.87|           0.0|    8772699.36| 1.126317843E7|      0|\n",
      "| 325|TRANSFER|1790422.14|      3129.54|           0.0|    4231642.96|     6022065.1|      0|\n",
      "| 226|TRANSFER| 202734.03|      72737.0|           0.0|    2436599.33|    2639333.35|      0|\n",
      "| 178|TRANSFER|   21265.4|     298159.0|      276893.6|     442474.75|     463740.15|      0|\n",
      "| 395|TRANSFER|1129651.91|     184666.0|           0.0|     499968.39|     1629620.3|      0|\n",
      "| 393|TRANSFER|1302660.22|      21885.0|           0.0|     864909.41|    2167569.63|      0|\n",
      "| 238|TRANSFER| 521672.55|     220417.1|           0.0|    1116968.02|    1638640.57|      0|\n",
      "| 301|TRANSFER| 360045.04|      50686.0|           0.0|      302200.4|     662245.44|      0|\n",
      "| 301|TRANSFER|1674287.16|      21160.0|           0.0|     169152.96|    1843440.12|      0|\n",
      "+----+--------+----------+-------------+--------------+--------------+--------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# duplicate the minority rows\n",
    "oversampled_df = minor_df.withColumn(\"dummy\", explode(array([lit(x) for x in a]))).drop('dummy')\n",
    "# combine both oversampled minority rows and previous majority rows\n",
    "data = major_df.unionAll(oversampled_df)\n",
    "data.show()\n",
    "data.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "3_Data_Scientist.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
